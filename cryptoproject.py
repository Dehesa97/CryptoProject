# -*- coding: utf-8 -*-
"""CryptoProject.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xxfQ1_kt_j96rwCjf3axPQqHwu9OXrx4
"""

!pip install OpenBlender pandas scikit-learn textblob pytrends transformers

import OpenBlender
from io import StringIO
import pandas as pd
import numpy as np
import json
import matplotlib.pyplot as plt
from textblob import TextBlob
from pytrends.request import TrendReq
pytrend = TrendReq()

#get Cryptoprices from Openblender


action = 'API_getObservationsFromDataset'

# ANCHOR: 'BTC to USD'

        
parameters = { 
    	'token':'623b09189516290d209a3c816Pri1VXVFj0nkKyb0QioflJ0kGFU5y',
	'id_user':'623b09189516290d209a3c81',
	'id_dataset':'5d4c3b789516290b02fe3e24',
	'date_filter':{"start_date":"2020-01-01T00:00:00.000Z","end_date":"2022-03-12"} 
}
        

df1 = pd.read_json(StringIO(json.dumps(OpenBlender.call(action, parameters)['sample'])), convert_dates=False, convert_axes=False).sort_values('timestamp', ascending=False)
df1.reset_index(drop=True, inplace=True)
df1['date'] = [OpenBlender.unixToDate(ts, timezone = 'GMT') for ts in df1.timestamp]

#find difference between opening and closing prices for each day using log,
#this will allow us to see whats identify big changes in relation to news 

df1['change'] = 100 * (np.log(df1['close']) - np.log(df1['open']))

#Create a positive or negative column so we can determine if the price change is good or bad
df1['prediction'] = [1 if change > 0 else 0 for change in df1['change']]


#blend with text
blend_source = {
                'id_dataset':'5ea2039095162936337156c9',
                'feature' : 'text'
            }
df_blend = OpenBlender.timeBlend( token = '623b09189516290d209a3c816Pri1VXVFj0nkKyb0QioflJ0kGFU5y',
                                  anchor_ts = df1.timestamp,
                                  blend_source = blend_source,
                                  blend_type = 'agg_in_intervals', #closest_observation
                                  interval_size = 60 * 60 * 24,
                                  direction = 'time_prior',
                                  interval_output = 'list',
                                  missing_values = 'raw')
df3 = pd.concat([df1, df_blend.loc[:, df_blend.columns != 'timestamp']], axis = 1)

#blend with nasdaq change
blend_source = {
    "id_dataset": "5d4c9b629516290b01c94904",
    "feature": "change",
}

df_blend = OpenBlender.timeBlend( token = '623b09189516290d209a3c816Pri1VXVFj0nkKyb0QioflJ0kGFU5y',
                                  anchor_ts = df1.timestamp,
                                  blend_source = blend_source,
                                  blend_type = 'closest_observation', 
                                  interval_size = 60 * 60 * 24,
                                  direction = 'time_prior',
                                  interval_output = 'list',
                                  missing_values = 'raw')
df4 = pd.concat([df3, df_blend.loc[:, df_blend.columns != 'timestamp']], axis = 1)

#rename columns
df4 = df4.rename(columns = {"BITCOIN_NE.text_COUNT_last1days": "no. articles", "BITCOIN_NE.text_last1days":"list of articles", "NQ100_PRICE.change_pre": "nasdaq change"})

#remove duplicates
df4 = df4.drop_duplicates(subset=['date'], keep='first')

df4.head()

x = df1['timestamp']
y = df1['close']
 
# plotting
plt.title("Bitcoin Price")
plt.xlabel("TimeStamp")
plt.ylabel("Price")
plt.plot(x, y, color ="blue")
plt.show()

from transformers import pipeline
sentiment_analysis = pipeline("sentiment-analysis",model='ProsusAI/finbert')
sent_an = pipeline("sentiment-analysis", model ="mrm8488/distilroberta-finetuned-financial-news-sentiment-analysis")

row_1=df4.iloc[2]
pos = 0
neg = 0


tex = sentiment_analysis(row_1['list of articles'])
print(tex, col)

from pyparsing.core import Each
def sentiment(textArray):
  neut = 0
  pos = 0
  neg = 0
  for each in textArray:
     tex = sentiment_analysis(each)[0]
     if tex['label'] == "neutral": 
        neut = neut + 1
     if tex['label'] == "positive": 
        pos = pos + 1
     if tex['label'] == "negative": 
        neg = neg + 1
  
  return([pos, neg, neut])

df4[['positive','negative', 'neutral']] = pd.DataFrame(df4.analysis.tolist(), index= df4.index)

from google.colab import drive
drive.mount('drive')

df4.to_csv('data.csv')
!cp data.csv "drive/My Drive/"

df4['analysis'] = df4['list of articles'].apply(lambda x: sentiment(x))
df4[['positive','negative', 'neutral']] = pd.DataFrame(df4.analysis.tolist(), index= df4.index)

df=pd.read_csv('drive/My Drive/data.csv')

df.head()

len(df.columns)

from pytrends import dailydata

df5 = dailydata.get_daily_data('bitcoin', 2020, 1, 2022, 3, geo = 'BR')

list=['Cryptocurrency', 'Bitcoin', 'Ethereum', 'Investing', 'Blockchain']

historicaldf = pytrend.get_historical_interest(list, year_start=2020, month_start=1, day_start=1, year_end=2022, month_end=3, day_end=23, cat=0, geo='', gprop='', sleep=0)

historicaldf.head()

historicaldf.plot(figsize=(20, 12))

historicaldf.plot(subplots=True, figsize=(20, 12))

import torch
import torch.nn as nn
import torch.nn.functional as F

class BTCPredictor(nn.Module):
    def __init__(self):
        super(BTCPredictor, self).__init__()
        self.input = nn.Linear(18,24)
        self.hidden1 = nn.Linear(24, 96)
        self.hidden2 = nn.Linear(96, 48)
        self.output = nn.Linear(48, 1)
        self.rel = nn.ReLU()
        
    def forward(self, x):
        x = self.input(x)
        x = self.rel(x)
        x = self.hidden1(x)
        x = self.rel(x)
        x = self.hidden2(x)
        x = self.rel(x)
        x = self.output(x)
        return x
    

def frame_train(net, df_inputs, targets, criterion, optimizer, counter=50):
    loss_plt = []       
    dflen = 100 
    
    for epoch in range(epochs):
        for (i, row), target in zip(df.iterrows(), targets):

          print(row, target)
          print('aa')
                           
          optimizer.zero_grad()
          input_tensor =  torch.tensor(row.to_list()).to(device)
          target_tensor = torch.tensor(0.).to(device)          
          output_tensor = net(input_tensor)
          
          loss = criterion(target_tensor, output_tensor)
          loss.backward()
          optimizer.step()
          loss_plt.append(loss.item())

        if (epoch%counter) == True:
            print('ep: ', epoch, 'loss: ', loss.item())
    
    plt.plot(loss_plt)


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')    

epochs = 1000
net = BTCPredictor().to(device)
criterion = nn.L1Loss().to(device)
#criterion = nn.MSELoss().to(device)
optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)

df=pd.read_csv('drive/My Drive/data.csv')

targets = df['change']
targets = targets.tolist()
targets.insert(0,0)
targets.pop()



frame_train(net, df, t_close, targets, criterion, optimizer)

import math
import string
import numpy as np
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
from textblob import TextBlob
from array import *


class ProcessText:

    def __init__(self):
        self.tfDict = dict()
        self.stemmedWords = []
        self.tfIDF = []
        self.lemmatized_words = ""
        self.sentences = []
        self.cleaned_text = []
        self.bag_of_words_mx = [[]]
        self.dot_product_mx = [[]]

    def clean_text(self, text_to_process):
        lemmatizer = WordNetLemmatizer()
        text = word_tokenize(text_to_process)
        ps = PorterStemmer()
        self.stopWordsRem = [word for word in text if not word in stopwords.words()]  # removes stop words from input
        for each in self.stopWordsRem:  # stems input words
            self.stemmedWords.append(ps.stem(each))
        self.lemmatized_words = [lemmatizer.lemmatize(w) for w in self.stemmedWords]  # lemmatizes each input word
        self.lemmatized_words = ' '.join(self.lemmatized_words)
        removed_punctuation = self.lemmatized_words.translate(
            str.maketrans('', '', string.punctuation))  # strips punctuation from string
        self.sentences = self.lemmatized_words.split(".")
        self.sentences = [''.join(c for c in s if c not in string.punctuation) for s in self.sentences]
        self.sentences = [s for s in self.sentences if s]
        self.cleaned_text = removed_punctuation.split(" ")
        self.cleaned_text = [s for s in self.cleaned_text if s]
        print(self.cleaned_text)

        for word in self.cleaned_text:
            self.tfDict[word] = 0

    def setTFIDF(self):  #use this to find relevant sentences / words
        for k, v in self.tfDict.items():
            termsCount = 0
            for word in self.cleaned_text:
                if(word == k):
                    termsCount += 1
            self.tfDict[k] = termsCount

        length = len(self.sentences)
        for k, v in self.tfDict.items():  # Finds the TF-IDF value of each word in the input text MIGHT need to add log to this
            documentsWithWord = 1
            for sentence in self.sentences:
                words = sentence.split(" ")
                for word in words:
                    if word == k:
                        documentsWithWord += 1
            self.tfDict[k] = math.log((length / documentsWithWord) * v)
        #for k, v in self.tfDict.items():
         #print(k, " : ", v)

    def bagOfWords(self):
        mx_y_size = len(self.tfDict)
        mx_x_size = len(self.sentences)
        self.badOfWords = np.zeros((mx_x_size, mx_y_size))
        for x in range(mx_x_size):
            vector = []
            sentence = self.sentences[x]
            counter = 0
            for k, v in self.tfDict.items():
                if k in sentence:
                    polarity = TextBlob(k).polarity
                    vector.append(polarity)
                else:
                    vector.append(-1)
            self.bag_of_words_mx.append(vector)
            print(vector)

            #for later optimizaiton, get the most important sentences and use them for bag of words

import os

from ProcessText import ProcessText


class Test:
    os.listdir()
    with open('/Users/maxcarrington/Documents/Group_project/Python_stuff/Input.txt', 'r') as file:
        data = file.read().replace('\n', ' ')
    object = ProcessText()
    ProcessText.clean_text(object, data)
    ProcessText.setTFIDF(object)
    ProcessText.bagOfWords(object)
    #ProcessText.dot_product(object)

"""# New Section"""